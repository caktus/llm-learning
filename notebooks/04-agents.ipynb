{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be57afbc-1ef0-4e96-8129-e8c9824d8830",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "AI agents are software systems that use AI to perform tasks on behalf of users. In this notebook, we'll experiment with creating and running AI agents using two different frameworks: OpenAI Agents and Pydantic AI.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Follow the README instructions to set up your Python environment.\n",
    "\n",
    "For the code examples that use local models, we'll be using Ollama's `llama3.2` model. If you haven't done so already, [install Ollama](https://ollama.com/download) on your computer and download the model with `ollama pull llama3.2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9ef96",
   "metadata": {},
   "source": [
    "## OpenAI Agents SDK\n",
    "\n",
    "The [OpenAI Agents SDK](https://github.com/openai/openai-agents-python/tree/main?tab=readme-ov-file#functions-example) is a tool that allows you to build agent-based workflows.\n",
    "\n",
    "This example shows how to build a simple agent that can call a function and return the result. Less code is required, and it automatically handles the conversation history using `last_response_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60634dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What's the weather in Tokyo?\n",
      "Response: The weather in Tokyo is currently stormy with a temperature of 46°F. If you need more details or a forecast, just let me know!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 2: How about the weather in New York?\n",
      "Response: The weather in New York is currently snowy with a temperature of 78°F. Let me know if you need more details or a forecast!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 3: What cities did I ask about?\n",
      "Response: You asked about the weather in the following cities:\n",
      "\n",
      "1. Tokyo\n",
      "2. New York\n",
      "\n",
      "If you want information about more cities, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Get the weather in a given city.\"\"\"\n",
    "    random_weather = random.choice([\"sunny\", \"cloudy\", \"rainy\", \"snowy\", \"partly cloudy\", \"stormy\"])\n",
    "    random_temp = random.randint(10, 100)\n",
    "\n",
    "    return {\n",
    "        \"conditions\": random_weather,\n",
    "        \"temperature\": random_temp,\n",
    "        \"city\": city,\n",
    "        \"units\": {\"temperature\": \"fahrenheit\"},\n",
    "    }\n",
    "\n",
    "\n",
    "# Create the agent with the weather tool\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a helpful weather agent. When asked about weather, use the get_weather tool to provide accurate information.\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the agent with conversation history.\"\"\"\n",
    "    # First query\n",
    "    result1 = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
    "    print(\"Query 1:\", \"What's the weather in Tokyo?\")\n",
    "    print(\"Response:\", result1.final_output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Second query - pass the previous response ID to maintain conversation\n",
    "    result2 = await Runner.run(\n",
    "        agent,\n",
    "        input=\"How about the weather in New York?\",\n",
    "        previous_response_id=result1.last_response_id,\n",
    "    )\n",
    "    print(\"Query 2:\", \"How about the weather in New York?\")\n",
    "    print(\"Response:\", result2.final_output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Third query - pass the previous response ID to maintain conversation\n",
    "    result3 = await Runner.run(\n",
    "        agent, input=\"What cities did I ask about?\", previous_response_id=result2.last_response_id\n",
    "    )\n",
    "    print(\"Query 3:\", \"What cities did I ask about?\")\n",
    "    print(\"Response:\", result3.final_output)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea92b0-c6e6-4a85-943f-d0ba5ed153b8",
   "metadata": {},
   "source": [
    "### Maintaining conversation history using a `Session`\n",
    "\n",
    "You can also use a `Session` object to maintain conversation history across multiple agent runs, instead of passing `previous_response_id` to `Runner.run()`. In the example below, we'll store the conversation history in an in-memory SQLite database, but you can pass a `db_path` argument to `SQLiteSession` for persistent storage. You can also implement your own session memory by creating a class that follows the `Session` protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858d54b-3764-431e-bbe6-cc4a17a97242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What's the weather in Tokyo?\n",
      "Response: The current weather in Tokyo is cloudy with a temperature of 73°F. If you need more details such as humidity or forecast, let me know!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 2: How about the weather in New York?\n",
      "Response: The weather in New York is currently cloudy with a temperature of 81°F. If you’d like more weather details or a forecast, just let me know!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 3: What cities did I ask about?\n",
      "Response: You asked about the weather in two cities: Tokyo and New York. If you want information about any other cities, just let me know!\n"
     ]
    }
   ],
   "source": [
    "from agents import SQLiteSession\n",
    "\n",
    "# Create a session instance\n",
    "session = SQLiteSession(session_id=\"my_session_id\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the agent with conversation history.\"\"\"\n",
    "    # First query\n",
    "    result1 = await Runner.run(agent, input=\"What's the weather in Tokyo?\", session=session)\n",
    "    print(\"Query 1:\", \"What's the weather in Tokyo?\")\n",
    "    print(\"Response:\", result1.final_output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Second query\n",
    "    result2 = await Runner.run(\n",
    "        agent,\n",
    "        input=\"How about the weather in New York?\",\n",
    "        session=session,\n",
    "    )\n",
    "    print(\"Query 2:\", \"How about the weather in New York?\")\n",
    "    print(\"Response:\", result2.final_output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Third query\n",
    "    result3 = await Runner.run(agent, input=\"What cities did I ask about?\", session=session)\n",
    "    print(\"Query 3:\", \"What cities did I ask about?\")\n",
    "    print(\"Response:\", result3.final_output)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350024f-233d-43a5-99a7-7ed4044437d4",
   "metadata": {},
   "source": [
    "### Using local models\n",
    "\n",
    "The SDK's integration with [LiteLLM](https://docs.litellm.ai/) allows us to use models from many other providers, including local Ollama models. We will use the same code as before, but provide an Ollama model to the `Agent` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4069a4-047d-46df-944e-4addb53d9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What's the weather in Tokyo?\n",
      "Response: The current weather in Tokyo is cloudy with a temperature of 37°F.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 2: How about the weather in New York?\n",
      "Response: The current weather in New York is cloudy with a temperature of 98°F.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 3: What cities did I ask about?\n",
      "Response: The cities you asked about are Tokyo and New York.\n"
     ]
    }
   ],
   "source": [
    "from agents import set_tracing_disabled\n",
    "from agents.extensions.models.litellm_provider import LitellmProvider\n",
    "\n",
    "# Disable tracing. By default, the SDK will send information about agent runs to\n",
    "# OpenAI. We don't necessarily want this when using local models.\n",
    "set_tracing_disabled(True)\n",
    "\n",
    "# Create a session instance\n",
    "session = SQLiteSession(session_id=\"my_session_id\")\n",
    "\n",
    "# Create the agent with the weather tool\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a helpful weather agent. When asked about weather, use the get_weather tool to provide accurate information.\",\n",
    "    tools=[get_weather],\n",
    "    model=LitellmProvider().get_model(\"ollama_chat/llama3.2\"),\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the agent with conversation history.\"\"\"\n",
    "    # First query\n",
    "    result1 = await Runner.run(agent, input=\"What's the weather in Tokyo?\", session=session)\n",
    "    print(\"Query 1:\", \"What's the weather in Tokyo?\")\n",
    "    print(\"Response:\", result1.final_output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Second query\n",
    "    result2 = await Runner.run(agent, input=\"How about the weather in New York?\", session=session)\n",
    "    print(\"Query 2:\", \"How about the weather in New York?\")\n",
    "    print(\"Response:\", result2.final_output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Third query\n",
    "    result3 = await Runner.run(agent, input=\"What cities did I ask about?\", session=session)\n",
    "    print(\"Query 3:\", \"What cities did I ask about?\")\n",
    "    print(\"Response:\", result3.final_output)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a136621",
   "metadata": {},
   "source": [
    "## Pydantic AI Agents\n",
    "\n",
    "[Pydantic AI](https://ai.pydantic.dev/) is another framework for creating and running AI agents. It also supports many different model providers, and its API is kinda similar to the OpenAI Agents SDK.\n",
    "\n",
    "Let's use it to recreate the same weather agent we created before, using OpenAI's `gpt-5-nano` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda19764-91ec-428d-98bf-07db4ed8cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What's the weather in Tokyo?\n",
      "Response: Tokyo — snowy. Temperature: 51°F (about 11°C).\n",
      "\n",
      "Tips: Snow can make roads and sidewalks slick—dress warmly and check transit updates if you’re traveling.\n",
      "\n",
      "Would you like an hourly forecast or a Celsius conversion?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 2: How about the weather in New York?\n",
      "Response: New York — stormy, 94°F (about 34°C).\n",
      "\n",
      "Would you like an hourly forecast or a Celsius conversion?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 3: What cities did I ask about?\n",
      "Response: Tokyo and New York.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Get the weather in a given city.\"\"\"\n",
    "    random_weather = random.choice([\"sunny\", \"cloudy\", \"rainy\", \"snowy\", \"partly cloudy\", \"stormy\"])\n",
    "    random_temp = random.randint(10, 100)\n",
    "\n",
    "    return {\n",
    "        \"conditions\": random_weather,\n",
    "        \"temperature\": random_temp,\n",
    "        \"city\": city,\n",
    "        \"units\": {\"temperature\": \"fahrenheit\"},\n",
    "    }\n",
    "\n",
    "\n",
    "# Create the agent with the weather tool\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a helpful weather agent. When asked about weather, use the get_weather tool to provide accurate information.\",\n",
    "    tools=[get_weather],\n",
    "    model=\"openai:gpt-5-nano\",\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the agent with conversation history.\"\"\"\n",
    "    # First query\n",
    "    result1 = await agent.run(\"What's the weather in Tokyo?\")\n",
    "    print(\"Query 1:\", \"What's the weather in Tokyo?\")\n",
    "    print(\"Response:\", result1.output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Second query - pass the message history so far to maintain conversation\n",
    "    result2 = await agent.run(\n",
    "        \"How about the weather in New York?\", message_history=result1.all_messages()\n",
    "    )\n",
    "    print(\"Query 2:\", \"How about the weather in New York?\")\n",
    "    print(\"Response:\", result2.output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Third query - pass the message history so far to maintain conversation\n",
    "    result3 = await agent.run(\n",
    "        \"What cities did I ask about?\", message_history=result2.all_messages()\n",
    "    )\n",
    "    print(\"Query 3:\", \"What cities did I ask about?\")\n",
    "    print(\"Response:\", result3.output)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49ca18",
   "metadata": {},
   "source": [
    "### Using local models\n",
    "\n",
    "We can also create our agent using an Ollama model. We'll only change the `model` argument in the `Agent` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6d91ac-7736-4fff-aa72-c04622e3a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What's the weather in Tokyo?\n",
      "Response: The current weather in Tokyo is mostly cloudy, with a temperature of 81°F.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 2: How about the weather in New York?\n",
      "Response: The current weather in New York is sunny, with a temperature of 57°F.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query 3: What cities did I ask about?\n",
      "Response: You asked about the weather in Tokyo and New York.\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "\n",
    "\n",
    "# Create the agent with the weather tool\n",
    "# We could also set the `model` to OpenAIChatModel(model_name=\"llama3.2\", provider=\"ollama\")\n",
    "# if we have a OLLAMA_BASE_URL environment variable (e.g. OLLAMA_BASE_URL=http://localhost:11434/v1)\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a helpful weather agent. When asked about weather, use the get_weather tool to provide accurate information.\",\n",
    "    tools=[get_weather],\n",
    "    model=OpenAIChatModel(\n",
    "        model_name=\"llama3.2\", provider=OllamaProvider(base_url=\"http://localhost:11434/v1\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the agent with conversation history.\"\"\"\n",
    "    # First query\n",
    "    result1 = await agent.run(\"What's the weather in Tokyo?\")\n",
    "    print(\"Query 1:\", \"What's the weather in Tokyo?\")\n",
    "    print(\"Response:\", result1.output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Second query - pass the message history so far to maintain conversation\n",
    "    result2 = await agent.run(\n",
    "        \"How about the weather in New York?\", message_history=result1.all_messages()\n",
    "    )\n",
    "    print(\"Query 2:\", \"How about the weather in New York?\")\n",
    "    print(\"Response:\", result2.output)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # Third query - pass the message history so far to maintain conversation\n",
    "    result3 = await agent.run(\n",
    "        \"What cities did I ask about?\", message_history=result2.all_messages()\n",
    "    )\n",
    "    print(\"Query 3:\", \"What cities did I ask about?\")\n",
    "    print(\"Response:\", result3.output)\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
