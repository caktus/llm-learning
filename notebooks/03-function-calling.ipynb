{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc7ee8b",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "Function calling allows you to define functions (with schemas) that the model can call as part of its response, enabling structured outputs, tool use, and more advanced workflows. \n",
    "\n",
    "For us, this seems like a first step towards our goal of enabling more complex interactions with LLMs, where they can not only generate text but also perform actions based on that text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89703288-53b8-4625-b253-478941ce8b6b",
   "metadata": {},
   "source": [
    "## Using OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abe1d5",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Make sure you have your OpenAI API key set up as described in the previous notebook. We'll use the Python SDK for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027b4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VS Code's Jupyter extension doesn't support loading .envrc, so if you're using VS Code, we load it here.\n",
    "\n",
    "from utils import load_envrc\n",
    "\n",
    "load_envrc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173ec48",
   "metadata": {},
   "source": [
    "### Defining a function schema\n",
    "\n",
    "We need to define a function that the model can use. Here we'll use a silly example function that \"cactifies\" a name. The schema for the function is defined in [JSON Schema format](https://json-schema.org/docs) and looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"cactify_name\",\n",
    "  \"type\": \"function\",\n",
    "  \"description\": \"Transforms a name into a fun, cactus-themed version.\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"name\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The name to be cactified.\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"name\"]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06e866",
   "metadata": {},
   "source": [
    "Following the [OpenAI function calling docs](https://platform.openai.com/docs/guides/function-calling), we can pass a `tools` parameter to the API endpoint to define functions the model can call. Here we use `curl` to demonstrate the raw API request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b7b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out curl_response\n",
    "\n",
    "curl https://api.openai.com/v1/responses -s \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
    "  -d '{\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"input\": [\n",
    "      {\"role\": \"user\", \"content\": \"What would my name, Colin, be if it were cactus-ified?\"}\n",
    "    ],\n",
    "    \"tools\": [\n",
    "      {\n",
    "        \"name\": \"cactify_name\",\n",
    "        \"type\": \"function\",\n",
    "        \"description\": \"Transforms a name into a fun, cactus-themed version.\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"name\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The name to be made cactus-like.\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"name\"]\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4353f624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'rs_0788c68d00cf12110068dd1753989c8196b6ba22ab5b539425'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>: <span style=\"font-weight: bold\">[]}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fc_0788c68d00cf12110068dd17551aa881968e5de7b4b0d2a5c9'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function_call'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'completed'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{\"name\":\"Colin\"}'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'call_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_Zbe1dnFFQ9TBey3ym4pxAweK'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cactify_name'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'rs_0788c68d00cf12110068dd1753989c8196b6ba22ab5b539425'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'reasoning'\u001b[0m, \u001b[32m'summary'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'id'\u001b[0m: \u001b[32m'fc_0788c68d00cf12110068dd17551aa881968e5de7b4b0d2a5c9'\u001b[0m,\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'function_call'\u001b[0m,\n",
       "        \u001b[32m'status'\u001b[0m: \u001b[32m'completed'\u001b[0m,\n",
       "        \u001b[32m'arguments'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"name\":\"Colin\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[32m'call_id'\u001b[0m: \u001b[32m'call_Zbe1dnFFQ9TBey3ym4pxAweK'\u001b[0m,\n",
       "        \u001b[32m'name'\u001b[0m: \u001b[32m'cactify_name'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from rich import print as rich_print\n",
    "\n",
    "data = json.loads(curl_response)  # noqa\n",
    "rich_print(data[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67a366",
   "metadata": {},
   "source": [
    "Reviewing the output, we see that the model decided to call our `cactify_name` function with the argument `\"Colin\"`. The model itself doesn't actually execute the function. It simply returns the function call in its response. It's up to us to handle the function execution and return the result if needed.\n",
    "\n",
    "It's easier to see this in action using the Python SDK, which we'll explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c187a",
   "metadata": {},
   "source": [
    "### Using the Python SDK\n",
    "\n",
    "Now let's see how to do the same thing using the Python SDK. We'll define the function schema as a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c534b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cactify_name_schema = {\n",
    "    \"name\": \"cactify_name\",\n",
    "    \"type\": \"function\",\n",
    "    \"description\": \"Transform a name into a fun, cactus-themed version.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"name\": {\"type\": \"string\", \"description\": \"The name to be cactified.\"}},\n",
    "        \"required\": [\"name\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4dbdf2",
   "metadata": {},
   "source": [
    "### Making a function call request\n",
    "\n",
    "Now, let's ask the model to cactify a name using the function we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a30d82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Response</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'resp_070610056460d7a00068dd1757c584819099b1d6c7af2a9bed'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1759319895.0</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">error</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">incomplete_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">instructions</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-5-nano-2025-08-07'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">output</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseReasoningItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'rs_070610056460d7a00068dd1758a0b881908a4d3ed8b687918f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">summary</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">encrypted_content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">status</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseFunctionToolCall</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"name\":\"Colin\"}'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_I7BVpmNmDnbYnv7JZOT1srtC'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cactify_name'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function_call'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fc_070610056460d7a00068dd1759a49c819089ad17329ab2013f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">status</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completed'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">parallel_tool_calls</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">tool_choice</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">tools</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FunctionTool</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cactify_name'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">parameters</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The name to be cactified.'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'additionalProperties'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">strict</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Transform a name into a fun, cactus-themed version.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">top_p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">background</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">conversation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">max_output_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">max_tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">previous_response_id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_cache_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">reasoning</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Reasoning</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">effort</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'medium'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">generate_summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">safety_identifier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">status</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completed'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseTextConfig</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">format</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseFormatText</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">verbosity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'medium'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">top_logprobs</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">truncation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'disabled'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">73</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InputTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">output_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">output_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">OutputTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">user</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">billing</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'payer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'developer'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">store</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'resp_070610056460d7a00068dd1757c584819099b1d6c7af2a9bed'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1759319895\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33merror\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mincomplete_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33minstructions\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-5-nano-2025-08-07'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'response'\u001b[0m,\n",
       "    \u001b[33moutput\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mResponseReasoningItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'rs_070610056460d7a00068dd1758a0b881908a4d3ed8b687918f'\u001b[0m,\n",
       "            \u001b[33msummary\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'reasoning'\u001b[0m,\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mencrypted_content\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mstatus\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mResponseFunctionToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"name\":\"Colin\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[33mcall_id\u001b[0m=\u001b[32m'call_I7BVpmNmDnbYnv7JZOT1srtC'\u001b[0m,\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'cactify_name'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'function_call'\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'fc_070610056460d7a00068dd1759a49c819089ad17329ab2013f'\u001b[0m,\n",
       "            \u001b[33mstatus\u001b[0m=\u001b[32m'completed'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mparallel_tool_calls\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mtemperature\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mtool_choice\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
       "    \u001b[33mtools\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mFunctionTool\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'cactify_name'\u001b[0m,\n",
       "            \u001b[33mparameters\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m,\n",
       "                \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'The name to be cactified.'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[32m'additionalProperties'\u001b[0m: \u001b[3;91mFalse\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mstrict\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'function'\u001b[0m,\n",
       "            \u001b[33mdescription\u001b[0m=\u001b[32m'Transform a name into a fun, cactus-themed version.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mtop_p\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mbackground\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mconversation\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmax_output_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmax_tool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mprevious_response_id\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mprompt\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mprompt_cache_key\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mreasoning\u001b[0m=\u001b[1;35mReasoning\u001b[0m\u001b[1m(\u001b[0m\u001b[33meffort\u001b[0m=\u001b[32m'medium'\u001b[0m, \u001b[33mgenerate_summary\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33msummary\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33msafety_identifier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mservice_tier\u001b[0m=\u001b[32m'default'\u001b[0m,\n",
       "    \u001b[33mstatus\u001b[0m=\u001b[32m'completed'\u001b[0m,\n",
       "    \u001b[33mtext\u001b[0m=\u001b[1;35mResponseTextConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mformat\u001b[0m=\u001b[1;35mResponseFormatText\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mverbosity\u001b[0m=\u001b[32m'medium'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mtop_logprobs\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "    \u001b[33mtruncation\u001b[0m=\u001b[32m'disabled'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mResponseUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_tokens\u001b[0m=\u001b[1;36m73\u001b[0m,\n",
       "        \u001b[33minput_tokens_details\u001b[0m=\u001b[1;35mInputTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33moutput_tokens\u001b[0m=\u001b[1;36m151\u001b[0m,\n",
       "        \u001b[33moutput_tokens_details\u001b[0m=\u001b[1;35mOutputTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m128\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m224\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33muser\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mbilling\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'payer'\u001b[0m: \u001b[32m'developer'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mstore\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client()\n",
    "\n",
    "input_list = [{\"role\": \"user\", \"content\": \"What would my name, Colin, be if it were cactus-ified?\"}]\n",
    "tools = [cactify_name_schema]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=input_list,\n",
    "    tools=tools,\n",
    ")\n",
    "rich_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596adf2e",
   "metadata": {},
   "source": [
    "Like the `curl` example, the response includes a function call with arguments. Let's extract and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315a04f9-76fa-4eb0-89ec-a82f4888a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cactify_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Makes a name more cactus-like.\n",
    "\n",
    "    Args:\n",
    "        name: The name to be cactified.\n",
    "\n",
    "    Returns:\n",
    "        The cactified version of the name.\n",
    "    \"\"\"\n",
    "\n",
    "    base_name = name\n",
    "\n",
    "    # Rule 1: If the name ends in 's' or 'x', remove it.\n",
    "    # Example: \"James\" -> \"Jame\", \"Alex\" -> \"Ale\"\n",
    "    if base_name.lower().endswith((\"s\", \"x\")):\n",
    "        base_name = base_name[:-1]\n",
    "\n",
    "    # Rule 2: If the name now ends in a vowel, remove it.\n",
    "    # Example: \"Jame\" -> \"Jam\", \"Mike\" -> \"Mik\", \"Anna\" -> \"Ann\"\n",
    "    if base_name and base_name.lower()[-1] in \"aeiou\":\n",
    "        base_name = base_name[:-1]\n",
    "\n",
    "    # Add the smoother suffix\n",
    "    return base_name + \"actus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011e0e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Colinactus\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response_output = response.output\n",
    "# The model decided to call our function\n",
    "function_call = response.output[1]\n",
    "# Load the arguments provided by the model to call the function\n",
    "args = json.loads(function_call.arguments)\n",
    "result = cactify_name(**args)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a6e25",
   "metadata": {},
   "source": [
    "Now that we have the output, we want to feed it back to the model to get a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b613c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What would my name, Colin, be if it were cactus-ified?'},\n",
       " ResponseReasoningItem(id='rs_070610056460d7a00068dd1758a0b881908a4d3ed8b687918f', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"name\":\"Colin\"}', call_id='call_I7BVpmNmDnbYnv7JZOT1srtC', name='cactify_name', type='function_call', id='fc_070610056460d7a00068dd1759a49c819089ad17329ab2013f', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_I7BVpmNmDnbYnv7JZOT1srtC',\n",
       "  'output': 'Colinactus'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the model's call to our function to the input list\n",
    "input_list += response_output\n",
    "# Append the function call output to the input list\n",
    "input_list.append(\n",
    "    {\"type\": \"function_call_output\", \"call_id\": function_call.call_id, \"output\": result}\n",
    ")\n",
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447aad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"Tell the user what their name would be if it were cactus-ified.\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2be1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseOutputText</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">annotations</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your cactus-ified name would be Colinactus! ðŸŒµ'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'output_text'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mResponseOutputText\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mannotations\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mtext\u001b[0m=\u001b[32m'Your cactus-ified name would be Colinactus! ðŸŒµ'\u001b[0m,\n",
       "    \u001b[33mtype\u001b[0m=\u001b[32m'output_text'\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich_print(response.output[0].content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2f7a9",
   "metadata": {},
   "source": [
    "There we go! We've successfully used function calling with the Python SDK to cactify a name. It's a bit silly, but it shows how function calling can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4204d4",
   "metadata": {},
   "source": [
    "### Detecting function calls and saving history\n",
    "\n",
    "We manually called the function and fed the output back to the model. In a real application, you'd want to automate this process. You could write a loop that checks if the model's response includes a function call, executes the function, and then sends the result back to the model until you get a final text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df229f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = []\n",
    "\n",
    "\n",
    "def prompt(user_input: str) -> str:\n",
    "    \"\"\"Prompt the model with the user input.\"\"\"\n",
    "\n",
    "    # Add the user input to the conversation history\n",
    "    all_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    # Prompt the model with the user input\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        tools=tools,\n",
    "        input=all_messages,\n",
    "    )\n",
    "\n",
    "    for event in response.output:\n",
    "        all_messages.append(event)\n",
    "\n",
    "        # There's a request from the model to use a tool\n",
    "        if event.type == \"function_call\":\n",
    "            function_name = event.name\n",
    "            function_args = json.loads(event.arguments)\n",
    "\n",
    "            # Execute the function based on its name\n",
    "            if function_name == \"cactify_name\":\n",
    "                result = cactify_name(function_args[\"name\"])\n",
    "\n",
    "            # Add the function call output to the all_messages list\n",
    "            # Use the exact format expected by the API\n",
    "            all_messages.append(\n",
    "                {\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": event.call_id,\n",
    "                    \"output\": json.dumps(result),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Now feed the function result back to the model\n",
    "            final_response = client.responses.create(\n",
    "                model=\"gpt-5-nano\",\n",
    "                instructions=\"Respond with what the name would be if it were cactus-ified in a sentence.\",\n",
    "                tools=tools,\n",
    "                input=all_messages,\n",
    "            )\n",
    "\n",
    "            for final_event in final_response.output:\n",
    "                if final_event.type == \"message\":\n",
    "                    text = final_event.content[0].text\n",
    "                    all_messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "                    return text\n",
    "\n",
    "        elif event.type == \"message\":\n",
    "            text = event.content[0].text\n",
    "            all_messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5d1fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colin would be Colinactus.\n"
     ]
    }
   ],
   "source": [
    "print(prompt(\"What would my name, Colin, be if it were cactus-ified?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0de325a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simon would be Simonactus.\n"
     ]
    }
   ],
   "source": [
    "print(prompt(\"What about Simon?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acdb916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colin and Simon.\n"
     ]
    }
   ],
   "source": [
    "print(prompt(\"What names did I ask about\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d42b4-1479-4397-924a-67ec833d1543",
   "metadata": {},
   "source": [
    "## Using local models with Ollama\n",
    "\n",
    "Not all Ollama models support function-calling, but we can find one that does by [filtering by \"tools\" on the Ollama search page](https://ollama.com/search?c=tools).\n",
    "\n",
    "Let's try the `llama3.2` model. If you haven't done so already, [install Ollama](https://ollama.com/download) on your computer and download the model with `ollama pull llama3.2`.\n",
    "\n",
    "### Defining a function schema\n",
    "\n",
    "We'll define our function using JSON Schema format. The schema is similar to what we defined for the OpenAI example, but slightly different:\n",
    "```json\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"cactify_name\",\n",
    "    \"description\": \"Transforms a name into a fun, cactus-themed version.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"name\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The name to be cactified.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"name\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Using `curl`\n",
    "\n",
    "Ollama's API is available at `http://localhost:11434` by default. Let's use `curl` to make a request to the `/api/chat/` endpoint and see the raw JSON response from the model. We'll provide our function schema in the `tools` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f21af96-da41-497b-af23-7f510debbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out curl_response\n",
    "\n",
    "curl http://localhost:11434/api/chat -s -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What would my name, Colin, be if it were cactus-ified?\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": false,\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"cactify_name\",\n",
    "        \"description\": \"Transforms a name into a fun, cactus-themed version.\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"name\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The name to be cactified.\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"name\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b835b2-628b-4d2d-a7a0-30aaaf370cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-10-01T11:58:41.961900899Z'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cactify_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Colin'</span><span style=\"font-weight: bold\">}}}]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7474827644</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51701877</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">184</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5693575941</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1728709049</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2'\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-10-01T11:58:41.961900899Z'\u001b[0m,\n",
       "    \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'cactify_name'\u001b[0m, \u001b[32m'arguments'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'Colin'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m7474827644\u001b[0m,\n",
       "    \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m51701877\u001b[0m,\n",
       "    \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m184\u001b[0m,\n",
       "    \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m5693575941\u001b[0m,\n",
       "    \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m20\u001b[0m,\n",
       "    \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m1728709049\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from rich import print as rich_print\n",
    "\n",
    "data = json.loads(curl_response)  # noqa\n",
    "rich_print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219c32c-7eac-4dac-8e5e-a18d17f67ea7",
   "metadata": {},
   "source": [
    "We see that the model decided to make a function call. As with the OpenAI example, the model simply tells us which function it would like to call and with which arguments. It's up to us to handle the function execution and pass back the result.\n",
    "\n",
    "### Using the `ollama` Python library\n",
    "\n",
    "We can make the same request using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd549d1d-1364-4aa1-8365-3c9a8717ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-10-01T11:58:44.12875799Z'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2088280143</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">load_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61502657</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">184</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98568995</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1927421880</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">thinking</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">images</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolCall</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cactify_name'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Colin'</span><span style=\"font-weight: bold\">}))]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-10-01T11:58:44.12875799Z'\u001b[0m,\n",
       "    \u001b[33mdone\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mdone_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mtotal_duration\u001b[0m=\u001b[1;36m2088280143\u001b[0m,\n",
       "    \u001b[33mload_duration\u001b[0m=\u001b[1;36m61502657\u001b[0m,\n",
       "    \u001b[33mprompt_eval_count\u001b[0m=\u001b[1;36m184\u001b[0m,\n",
       "    \u001b[33mprompt_eval_duration\u001b[0m=\u001b[1;36m98568995\u001b[0m,\n",
       "    \u001b[33meval_count\u001b[0m=\u001b[1;36m20\u001b[0m,\n",
       "    \u001b[33meval_duration\u001b[0m=\u001b[1;36m1927421880\u001b[0m,\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mthinking\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mimages\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mToolCall\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'cactify_name'\u001b[0m, \u001b[33marguments\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'Colin'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "input_list = [{\"role\": \"user\", \"content\": \"What would my name, Colin, be if it were cactus-ified?\"}]\n",
    "cactify_name_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"cactify_name\",\n",
    "        \"description\": \"Transforms a name into a fun, cactus-themed version.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"name\": {\"type\": \"string\", \"description\": \"The name to be cactified.\"}},\n",
    "            \"required\": [\"name\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "tools = [cactify_name_schema]\n",
    "\n",
    "response = ollama.chat(\n",
    "    \"llama3.2\",\n",
    "    messages=input_list,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "rich_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0e037-e5e8-4efd-98fb-2e719e637209",
   "metadata": {},
   "source": [
    "We can also pass the actual Python function in the `tools` argument and Ollama will generate the schema for us in the background. This makes it easy to use an existing function as a tool. For best results, it's recommended to provide type annotations for parameters and return values, and add a [Google-style docstring](https://google.github.io/styleguide/pyguide.html#doc-function-raises). We've already done this for our `cactify_name` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7edbfb9c-ae24-4096-8a5b-439d0cb70604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-10-01T11:58:49.688111262Z'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5545610881</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">load_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69897155</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">179</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3538477099</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1936451125</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">thinking</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">images</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolCall</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cactify_name'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Colin'</span><span style=\"font-weight: bold\">}))]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-10-01T11:58:49.688111262Z'\u001b[0m,\n",
       "    \u001b[33mdone\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mdone_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mtotal_duration\u001b[0m=\u001b[1;36m5545610881\u001b[0m,\n",
       "    \u001b[33mload_duration\u001b[0m=\u001b[1;36m69897155\u001b[0m,\n",
       "    \u001b[33mprompt_eval_count\u001b[0m=\u001b[1;36m179\u001b[0m,\n",
       "    \u001b[33mprompt_eval_duration\u001b[0m=\u001b[1;36m3538477099\u001b[0m,\n",
       "    \u001b[33meval_count\u001b[0m=\u001b[1;36m20\u001b[0m,\n",
       "    \u001b[33meval_duration\u001b[0m=\u001b[1;36m1936451125\u001b[0m,\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mthinking\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mimages\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mToolCall\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'cactify_name'\u001b[0m, \u001b[33marguments\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'Colin'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    \"llama3.2\",\n",
    "    messages=input_list,\n",
    "    tools=[cactify_name],\n",
    ")\n",
    "\n",
    "rich_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71742c7e-a78d-4b7e-8e80-dd270a754257",
   "metadata": {},
   "source": [
    "Let's execute the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a8494c-b3fd-4271-ad3b-98f2a00c6bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Colinactus\n"
     ]
    }
   ],
   "source": [
    "function_call = response.message.tool_calls[0].function\n",
    "# The arguments are already a Python dict, not JSON\n",
    "result = cactify_name(**function_call.arguments)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecf466-e987-4838-a0bf-413c3a05e508",
   "metadata": {},
   "source": [
    "Then we'll feed back the result to the model to get a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "903cb505-b301-4051-9733-f79bb3bbf846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What would my name, Colin, be if it were cactus-ified?'},\n",
       " Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=[ToolCall(function=Function(name='cactify_name', arguments={'name': 'Colin'}))]),\n",
       " {'role': 'tool', 'content': 'Colinactus'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the model's response to the input_list first, for conversation history\n",
    "input_list.append(response.message)\n",
    "input_list.append({\"role\": \"tool\", \"content\": result})\n",
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64050969-9a2e-476d-be82-80cdc36289c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-10-01T11:58:54.744924972Z'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5035987618</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">load_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">71600567</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">105</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3135668337</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1826394100</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The cactified version of your name, Colin, is indeed \"Colinactus\".'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">thinking</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">images</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-10-01T11:58:54.744924972Z'\u001b[0m,\n",
       "    \u001b[33mdone\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mdone_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mtotal_duration\u001b[0m=\u001b[1;36m5035987618\u001b[0m,\n",
       "    \u001b[33mload_duration\u001b[0m=\u001b[1;36m71600567\u001b[0m,\n",
       "    \u001b[33mprompt_eval_count\u001b[0m=\u001b[1;36m105\u001b[0m,\n",
       "    \u001b[33mprompt_eval_duration\u001b[0m=\u001b[1;36m3135668337\u001b[0m,\n",
       "    \u001b[33meval_count\u001b[0m=\u001b[1;36m19\u001b[0m,\n",
       "    \u001b[33meval_duration\u001b[0m=\u001b[1;36m1826394100\u001b[0m,\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'The cactified version of your name, Colin, is indeed \"Colinactus\".'\u001b[0m,\n",
       "        \u001b[33mthinking\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mimages\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    \"llama3.2\",\n",
    "    messages=input_list,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "rich_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea789603-d9b7-4114-8b39-2c85d68fa00c",
   "metadata": {},
   "source": [
    "### Detecting function calls and saving history\n",
    "\n",
    "Let's create a function that automates the whole process of prompting, detecting a function call, executing the function call, and sending it's result back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d8a3a3e-14dd-43a8-95f1-cf9b6b3e9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_messages = []\n",
    "\n",
    "\n",
    "def prompt_local(user_input: str) -> str:\n",
    "    \"\"\"Prompt the model with the user input.\"\"\"\n",
    "\n",
    "    # Add the user input to the conversation history\n",
    "    ollama_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    # Prompt the model with the user input\n",
    "    response = ollama.chat(\n",
    "        \"llama3.2\",\n",
    "        messages=ollama_messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    if response.message.tool_calls:\n",
    "        # There's a request from the model to use one or more tools\n",
    "        ollama_messages.append(response.message)\n",
    "\n",
    "        for tool_call in response.message.tool_calls:\n",
    "            # Execute the function based on its name\n",
    "            if tool_call.function.name == \"cactify_name\":\n",
    "                result = cactify_name(**tool_call.function.arguments)\n",
    "                # Add the function call output to the messages list\n",
    "                ollama_messages.append(\n",
    "                    {\"role\": \"tool\", \"content\": result, \"tool_name\": \"cactify_name\"}\n",
    "                )\n",
    "\n",
    "        # Now feed the function result back to the model\n",
    "        final_response = ollama.chat(\"llama3.2\", messages=ollama_messages, tools=tools)\n",
    "\n",
    "        ollama_messages.append(final_response.message)\n",
    "\n",
    "        return final_response.message.content\n",
    "\n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50c0f749-e170-4a4c-8851-dff05d9f557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the tool call response, I've formed an answer to your original question: If Colin's name were cactus-ified, it would be Colinactus.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_local(\"What would my name, Colin, be if it were cactus-ified?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "951eed36-6ebb-4f57-8009-49c784658a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the tool call response, I've formed an answer to your original question: If Simon's name were cactus-ified, it would be Simonactus.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_local(\"What about Simon?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7737326f-bf23-4251-8bb9-a68a482b7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on our conversation, you asked about cactifying the names Colin and Simon. The resulting names are Colinactus and Simonactus, respectively.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_local(\"What names did I ask about?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73da83-883c-4890-af0c-0500a1d39845",
   "metadata": {},
   "source": [
    "I found that sometimes the model would ignore the result of the function call and come up with its own cactus-ified name. It would also sometimes attempt to call the function with incorrect arguments. For example, on the \"What about Simon?\" prompt, it would specify the function argument to be \"Simonactus\", maybe basing it on the previous prompt's result. Or on \"What names did I ask about?\", it would try to call the function with a JSON array like `[\"Colin\", \"Simon\"]`. The results may be better or more consistent with larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b9427-85eb-4452-9093-e9df0e7c719b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
